{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "\n",
    "# SciPy-related functions\n",
    "from scipy.stats import multivariate_normal, exponnorm, norm\n",
    "from scipy.special import erf, erfc \n",
    "from scipy.stats.qmc import PoissonDisk\n",
    "\n",
    "# scikit-learn-related functions\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "\n",
    "# Suppress scientific notation for floats\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# Ensure repearable randomness\n",
    "RNG = np.random.default_rng(2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Attention Simulator for Immersive Rooms\n",
    "\n",
    "This notebook describes an exploratory simulator that captures the attentional distribution in immersive rooms using a physically-based model of human agent movement. Although the simulator will be eventually incorporated in the Unity game engine, the mathematical definition of agent attention is best prototyped in isolation using NumPy. Therefore, to help clarifying thoughts, I will describe here in mathematical terms how agent attention is represented in the immersive room, and how the movement of agent group is imprinted as the room's attentional memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 0: Some Background\n",
    "\n",
    "The starting point of this simulator comes from the theory of proxemics, proposed in Edward T. Hall's [*The Hidden Dimension*](https://archive.org/details/hiddendimensionhall00hall/page/n9/mode/2up). In his book, he specified a series of distance-based measures that thresholds the spatial boundaries of human interactions. This has inspired some coarse simulation of human agent behavior in early developments of physically-realistic virtual environment (see [Yan and Kalay, 2006](https://link.springer.com/chapter/10.1007/978-1-4020-5131-9_4)). Here, we adopt it conceptually as a toy model, with some variations.\n",
    "\n",
    "For this simulator, we do not seek to replicate exactly the distance thresholds outlined in these works. Instead, to account for individual differences in the future, we treat these thresholds more liberally. We interpret attention as *the probability that an agent would physically interact with a surrounding target based upon where the agent is located and oriented*. The attention of these agents can then be modeled as a continuous probability distribution in space. Optionally, the discrete thresholds can be used as data points for model fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Agent Representation\n",
    "\n",
    "We first consider the 2D geometries of agents in the room (seen from the top) as the basis of agent representation. The room has a pre-defined interaction boundary $\\Omega$ that constrains its physical dimensions. Let $a = 1, 2, ..., A$ be the agents present in the immersive room. They each have positions $\\mathbf{x}_a = (x_a, y_a)$ and orientations $\\theta_a$. These parameters allow us to create two types of agent-based attentional distributions in the room, $w_a$:\n",
    "\n",
    "<p align=\"center\"><img src=\"../assets/attentional_weights.png\" width=\"400\"></p>\n",
    "\n",
    "1) Proximity-based attentional weight, $w(x, y)$. This is a function of attentional probability based solely on how closely gathered agents are, as per Hall's original theory. For each agent $a$, this can be represented as a 2D spherical Gaussian distribution in the room:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    w_a(x, y) = \\exp \\left(-\\frac{(x - x_a)^2 + (y - y_a)^2}{2\\sigma_a^2} \\right).\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proximity-based attentional weight\n",
    "def proximity_based_weight(base, mu, sigma=1):\n",
    "    \"\"\"\n",
    "    Proximity-based attentional_weight. \n",
    "    This is done in deterministic calculation.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    position    : array-like\n",
    "        Spatial positions of the agent as the mean of this weight in space.\n",
    "\n",
    "    spread      : array-like\n",
    "        Distance-based spread of the weight as spherical covariance matrix.\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    Attentional weight of a single agent in free space based solely upon proximity.\n",
    "    \"\"\"\n",
    "    return multivariate_normal(mu, sigma).pdf(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing proximity-based attentional weight for a single agent\n",
    "\n",
    "# Define physical dimension and granularity of spatial discretization\n",
    "l, w = np.array([4., 4.])\n",
    "hl, hw = l/2, w/2\n",
    "d = 0.05\n",
    "\n",
    "# Create a data grid for the footprint\n",
    "x, y = np.mgrid[-hl:hl+d:d, -hw:hw+d:d]\n",
    "grid = np.dstack((x, y))\n",
    "\n",
    "# Generate agent positions\n",
    "weight = proximity_based_weight(grid, mu=[0, 0])\n",
    "\n",
    "# Plotting this\n",
    "f, ax = plt.subplots(1, 1, figsize=(hl, hw))\n",
    "f = plt.contourf(x, y, weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Orientation-based attentional weight, $w(x, y, r, \\theta)$. This is a function based on both the agents' distances and their orientations. Different from the proximity-based attentional weight, this attentional weight is biased towards the front side of the agent, where peripheral information on the back side contributes less. \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    w_a(x, y, r, \\theta) = \\exp \\left(-\\frac{(x - x_a - r \\cos \\theta_a)^2 + (y - y_a - r \\sin \\theta_a)^2}{2\\sigma_a^2} \\right).\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    r(z) = 1 - z, \\quad z \\in [0, 1].\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Note: $z$ is on the same axis as $w_a$. This is where I'm stuck. Although this weight is visually intuitive, the exact mathematical formulation here is not very clear. As a placeholder, I will use exponentially-modified normal distribution, for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orientation-based attention weight:\n",
    "def orientation_based_weight(base, r, theta, mu, sigma=1):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    \n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing proximity-based attentional weight for a single agent\n",
    "\n",
    "# Define physical dimension and granularity of spatial discretization\n",
    "l, w = np.array([4., 4.])\n",
    "hl, hw = l/2, w/2\n",
    "d = 0.05\n",
    "\n",
    "# Create a data grid for the room's footprint\n",
    "x, y = np.mgrid[-hl:hl+d:d, -hw:hw+d:d]\n",
    "grid = np.dstack((x, y))\n",
    "\n",
    "# Generate agent positions\n",
    "# weight = orientation_based_weight(grid, position=[0, 0])\n",
    "\n",
    "# Plotting this\n",
    "#f, ax = plt.subplots(1, 1, figsize=(hl, hw))\n",
    "#f = plt.contourf(x, y, weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imprint of these attention distribution can be seen as as a Gaussian mixture:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    w(x, y) = \\sum_a w_a(x, y)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "We use Poisson disc sampling as the initial condition for the agents' attention distribution in the room, implemented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_multiple_agent_positions(radius=0.5, N=24):\n",
    "    from scipy.stats.qmc import PoissonDisk\n",
    "    \"\"\"\n",
    "    Get multiple agent positions in random, using the Poisson Disc method.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    radius  : float\n",
    "        Distance between each agent position.\n",
    "\n",
    "    N       : int\n",
    "        Maximum number of agent positions.\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    An array of agent positions.\n",
    "    \"\"\"\n",
    "    engine = PoissonDisk(2, radius=radius)\n",
    "    return engine.random(N)\n",
    "    \n",
    "def sample_proximal_weights(base, positions, spread=1):\n",
    "    \"\"\"\n",
    "    Generate a map of weight that represents the spatial distribution of \n",
    "    proximity between agents as a Gaussian mixture.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    base        : array-like\n",
    "        Spatial grid of the room.\n",
    "\n",
    "    positions   : array-like\n",
    "        Sampled array of agent positions.\n",
    "\n",
    "    spread      : int\n",
    "        Scale factor of spatial attentional variance for each agent.\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    A spatial distribution of attentional weight.\n",
    "    \"\"\"\n",
    "    weights = np.zeros(base.shape[:2])\n",
    "    for p in positions:\n",
    "        weights += proximity_based_weight(base, p, np.eye(2) * spread)\n",
    "    return weights / positions.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contour plot below shows the attentional distribution in ROIS as a snapshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the attentional weights\n",
    "\n",
    "# Define physical dimension and granularity of spatial discretization\n",
    "l, w = np.array([8., 10.])\n",
    "hl, hw = l/2, w/2\n",
    "d = 0.05\n",
    "\n",
    "# Create a data grid for the footprint\n",
    "x, y = np.mgrid[-hl:hl+d:d, -hw:hw+d:d]\n",
    "grid = np.dstack((x, y))\n",
    "\n",
    "# Generate agent positions\n",
    "positions = sample_multiple_agent_positions(radius=0.25) * (l, w) - (hl, hw)\n",
    "proximal_weights = sample_proximal_weights(grid, positions, spread=0.75)\n",
    "\n",
    "# Plotting this\n",
    "f, ax = plt.subplots(1, 1, figsize=(hl, hw))\n",
    "f = plt.contourf(x, y, proximal_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Agent Movement\n",
    "\n",
    "For the simplest simulation of agent movement, we use a simple multi-point attractor framework. The agents simply walk directly to the object closest to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attract(agent_positions, object_positions, bound, dt = 0.001, max_time = 1.):\n",
    "    from scipy.spatial import distance\n",
    "    \"\"\"\n",
    "    A simple attractor framework for simulating agent movement. \n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    agent_positions : np.array\n",
    "        An array of agent positions.\n",
    "    \n",
    "    object_positions : np.array\n",
    "        An array of object positions.\n",
    "\n",
    "    dt : float\n",
    "        Time interval per update.\n",
    "    \n",
    "    max_time : float\n",
    "        Duration of simulation.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    new_agent_positions : np.array\n",
    "        An array of agent positions at the end of the simulation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the count of agent and objects\n",
    "    num_agents, num_objects = agent_positions.shape[0], object_positions.shape[0]\n",
    "    new_agent_positions = agent_positions.copy()\n",
    "\n",
    "    # Initialize a NumPy array of distances and angles\n",
    "    distances = np.zeros((num_agents, num_objects))\n",
    "    target_ids = np.zeros(num_agents, dtype=np.int16)\n",
    "\n",
    "    t = 0.\n",
    "    # I will vectorize / refactor this later.\n",
    "    while t < max_time:\n",
    "\n",
    "        # Compute distances\n",
    "        for a in range(num_agents):\n",
    "            for b in range(num_objects):\n",
    "                distances[a,b] = distance.euclidean(agent_positions[a], object_positions[b])\n",
    "                \n",
    "                # Identify targets for each agent\n",
    "                target_ids[a] = np.argmin(distances[a])\n",
    "            \n",
    "                #Update positions\n",
    "                if np.abs(new_agent_positions[a, 0]) <= bound[0] and np.abs(new_agent_positions[a, 1]) <= bound[1]:\n",
    "                    new_agent_positions[a] += np.subtract(object_positions[target_ids[a]], agent_positions[a]) * dt\n",
    "    \n",
    "        t += dt\n",
    "    return new_agent_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_starts = sample_multiple_agent_positions(radius=0.25) * (l, w) - (hl, hw)\n",
    "objects = np.array([[1., 10.], [7., 1.], [-2., -9.], [-7., -2.], [0., 0.]])\n",
    "\n",
    "agent_ends = attract(agent_starts, objects, bound=[4, 5])\n",
    "\n",
    "start_weights = sample_proximal_weights(grid, agent_starts, spread=0.75)\n",
    "end_weights = sample_proximal_weights(grid, agent_ends, spread=0.75)\n",
    "\n",
    "# Plotting this\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(2*hl, hw))\n",
    "plt.suptitle(\"Simple Target Approach\")\n",
    "ax1.contourf(x, y, start_weights)\n",
    "ax2.contourf(x, y, end_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Can this be turned into a 2D random walk?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Need to ask Stefan about good ways to make animations.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Clustering\n",
    "\n",
    "We can use K-mean clustering to extract attentional agent groups. However, K-mean clustering does not take into account the variance of the clusters. This means that we cannot accurately describe the size of these clusters, which is important when we try to assess how many people are within a cluster. \n",
    "\n",
    "The more appropriate clustering algorithm would be the [expectation-maximization (EM) algorithm](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm#Gaussian_mixture) for a Gaussian mixture model. The EM algorithm is also advantageous because of its probabilistic nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_cluster(weights):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To take it one step further, we can use Variational Bayesian (VB) method for the same thing. The difference between VB and EM is exactly what to use for estimation. While EM estimates from sampled data points, VB estimates the entire distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Trajectory Simulation\n",
    "\n",
    "Comparing trajectories using [dynamic time warping (DTW)](https://academic.oup.com/jrsssc/article/67/5/1147/7058405?login=false) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
